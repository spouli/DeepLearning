{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d0de9b6",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis of IMDB Movie Reviews**\n",
    "\n",
    "</br>\n",
    "\n",
    "**Dataset**\n",
    "</br>\n",
    "\n",
    "The IMDb Dataset of 50K Movie Reviews, is a popular dataset commonly used for sentiment analysis and natural language processing tasks. The dataset consists of 50,000 movie reviews, with 25,000 reviews labeled as positive and 25,000 as negative\n",
    "</br>\n",
    "\n",
    "Dataset Source: [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?datasetId=134715&searchQuery=pytor)\n",
    "</br>\n",
    "\n",
    "**The Problem Statement**\n",
    "</br>\n",
    "\n",
    "Predict the number of positive and negative reviews based on sentiments by using deep learning techniques.\n",
    "\n",
    "**To approach this problem, we've followed the below outline:**\n",
    "\n",
    "- **Data preprocessing:** applied in the notebook called _\"Data_preprocessing_notebook\"_\n",
    "</br>\n",
    "\n",
    "- **Word embedding:** We've converted the preprocessed text into a numerical representation that can be understood by deep learning models, using word embeddings, such as Word2Vec or GloVe, to represent words as dense vectors in a continuous vector space.\n",
    "</br>\n",
    "\n",
    "- **Model selection:** Choose a suitable deep learning model architecture including recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and convolutional neural networks (CNNs). \n",
    "</br>\n",
    "\n",
    "- **Model training:** Split our dataset into training and validation sets.\n",
    "</br>\n",
    "- **Model evaluation**\n",
    "</br>\n",
    "- **Model refinement**\n",
    "</br>\n",
    "\n",
    "**(Initial) Attributes**:\n",
    "\n",
    "* Review\n",
    "* Sentiment\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9279f6e6",
   "metadata": {},
   "source": [
    "## All the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f47b4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# import to \"ignore\" warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# imports for data manipulation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# imports for data visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud # need local import\n",
    "\n",
    "\n",
    "# import pytorch (framework for building deep learning models) || need local import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import keras (framework for building deep learning models) || need local import\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout , LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "# imports from sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import gensim # need local import\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import nltk\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b80ed48",
   "metadata": {},
   "source": [
    "## Load the csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f059b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch oz episod hook right ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic famili littl boy jake think zombi closet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one review mention watch oz episod hook right ...          1\n",
       "1  wonder littl product film techniqu unassum old...          1\n",
       "2  thought wonder way spend time hot summer weeke...          1\n",
       "3  basic famili littl boy jake think zombi closet...          0\n",
       "4  petter mattei love time money visual stun film...          1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_csv('imdb_clean_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7cc1cb7",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7b1fa1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (39665,)\n",
      "Shape of test data: (9917,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Shape of train data: {X_train.shape}')\n",
    "print(f'Shape of test data: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "51920aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------X_train_tokenized: \n",
      "\n",
      "[['realli', 'like', 'movi', 'empor', 'new', 'groov', 'watch', 'like', 'come', 'home', 'see', 'wife', 'relat', 'llama', 'serious', 'movi', 'bad', 'like', 'club', 'dread', 'super', 'trooper', 'suppos', 'write', 'line', 'even', 'know', 'els', 'say', 'laugh', 'coupl', 'time', 'drink', 'movi', 'like', 'least', 'funni', 'drunk', 'mayb', 'llama', 'funni', 'regular', 'cartoon', 'peopl', 'either', 'way', 'stick', 'empor', 'new', 'groov', 'want', 'funni', 'cartoon', 'llama', 'theme', 'movi', 'line', 'line', 'right']]\n",
      "\n",
      "\n",
      "--------------X_test_tokenized: \n",
      "\n",
      "[['soul', 'plane', 'horribl', 'attempt', 'comedi', 'appeal', 'peopl', 'thick', 'skull', 'bloodshot', 'eye', 'furri', 'pawn', 'plot', 'incoher', 'also', 'non', 'exist', 'act', 'mostli', 'sub', 'sub', 'par', 'gang', 'highli', 'moron', 'dread', 'charact', 'thrown', 'bad', 'measur', 'joke', 'often', 'spot', 'mile', 'ahead', 'almost', 'never', 'even', 'bit', 'amus', 'movi', 'lack', 'structur', 'full', 'racial', 'stereotyp', 'must', 'seem', 'old', 'even', 'fifti', 'thing', 'realli', 'go', 'pretti', 'ladi', 'realli', 'want', 'rent', 'someth', 'adult', 'section', 'ok', 'hardli', 'see', 'anyth', 'recommend', 'sinc', 'probabl', 'lot', 'better', 'product', 'time', 'chase', 'rat', 'sledgehamm', 'invent', 'waterproof', 'teabag', 'whatev']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the reviews\n",
    "X_train_tokenized = [word_tokenize(review) for review in X_train]\n",
    "X_test_tokenized = [word_tokenize(review) for review in X_test]\n",
    "\n",
    "print('--------------X_train_tokenized: \\n')\n",
    "print(X_train_tokenized[:1])\n",
    "print('\\n')\n",
    "print('--------------X_test_tokenized: \\n')\n",
    "print(X_test_tokenized[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55e02dd1",
   "metadata": {},
   "source": [
    "## Word embedding using Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2361729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn word embeddings specific to the training set, the Word2Vec model is trained on the X_train_tokenized data, \n",
    "# which consists of tokenized reviews from the training set. \n",
    "# This division ensures a realistic assessment and reduces information leaking from the testing set. \n",
    "# The test set is handled as new data, giving a precise evaluation of the model's performance on novel occurrences. \n",
    "# Word embeddings are created for the testing data using the trained model.\n",
    "\n",
    "model = Word2Vec(sentences=X_train_tokenized, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e11c22e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 63780\n",
      "avg_vector_size: 100\n",
      "num_reviews: 39665\n",
      "max_review_length: 1135\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(model.wv)\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "\n",
    "# Get the average word vector size\n",
    "avg_vector_size = model.vector_size\n",
    "print(f\"avg_vector_size: {avg_vector_size}\")\n",
    "\n",
    "# Get the total number of reviews in the training set\n",
    "num_reviews = len(X_train_tokenized)\n",
    "print(f\"num_reviews: {num_reviews}\")\n",
    "\n",
    "# Get the maximum number of words in a review\n",
    "max_review_length = max(len(review) for review in X_train_tokenized)\n",
    "print(f\"max_review_length: {max_review_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "af5f9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embeddings for training data\n",
    "X_train_word_embeddings = []\n",
    "for review in X_train_tokenized:\n",
    "    review_embedding = []\n",
    "    for word in review:\n",
    "        if word in model.wv:  # Check if the word has a word vector in the Word2Vec model's vocabulary\n",
    "            word_embedding = model.wv[word]  # Retrieve the word vector for the word\n",
    "            review_embedding.append(word_embedding)  # Add the word vector to the review_embedding list\n",
    "    if review_embedding: #check if the review_embedding list is not empty.\n",
    "        review_embedding_avg = sum(review_embedding) / len(review_embedding)  # Calculate the average embedding\n",
    "        X_train_word_embeddings.append(review_embedding_avg)  # Append the average embedding to X_train_word_embeddings\n",
    "    else:\n",
    "        X_train_word_embeddings.append([])  # Append an empty list if no word vectors were found for the review\n",
    "\n",
    "# Generate word embeddings for testing data\n",
    "X_test_word_embeddings = []\n",
    "for review in X_test_tokenized:\n",
    "    review_embedding = []\n",
    "    for word in review:\n",
    "        if word in model.wv:  # Check if the word has a word vector in the Word2Vec model's vocabulary\n",
    "            word_embedding = model.wv[word]  # Retrieve the word vector for the word\n",
    "            review_embedding.append(word_embedding)  # Add the word vector to the review_embedding list\n",
    "    if review_embedding: #check if the review_embedding list is not empty.\n",
    "        review_embedding_avg = sum(review_embedding) / len(review_embedding)  # Calculate the average embedding\n",
    "        X_test_word_embeddings.append(review_embedding_avg)  # Append the average embedding to X_test_word_embeddings\n",
    "    else:\n",
    "        X_test_word_embeddings.append([])  # Append an empty list if no word vectors were found for the review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2fcfd668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed embeddings (training data): 0\n",
      "Number of removed embeddings (testing data): 0\n",
      "\n",
      "\n",
      "--------------X_train_word_embeddings: \n",
      "\n",
      "[array([ 0.11843939,  0.40737468, -0.31806594, -0.413338  , -0.32696098,\n",
      "       -0.81214046,  0.42219087,  0.4818255 , -0.01088629, -0.4130508 ,\n",
      "        0.94538206,  0.373542  ,  0.7279831 , -0.53524673, -0.4170448 ,\n",
      "       -0.02434385,  0.3920842 , -0.5743434 , -0.15548031, -0.2712898 ,\n",
      "        0.14474407, -0.51100916,  0.01840197, -0.1433615 , -0.7358261 ,\n",
      "       -0.06844264,  0.01321736,  0.34234086, -0.06639291,  0.01833295,\n",
      "        0.4464348 , -0.51792115,  0.46142694, -0.50904745,  0.35335052,\n",
      "        1.0143116 ,  0.56808984, -0.07859233, -0.46526942, -0.51201195,\n",
      "       -0.03492953,  0.5546405 ,  0.28440964,  0.0626305 ,  1.1908377 ,\n",
      "        0.32133946,  0.3726599 ,  0.530576  , -0.09281716, -0.19503234,\n",
      "        0.08419336,  0.4327711 , -0.407829  ,  0.10671559, -0.33138898,\n",
      "       -0.38468325,  0.26527426,  0.9788681 , -0.55170894, -0.66979676,\n",
      "       -0.67829305, -0.4808703 ,  0.8733946 , -0.1672293 , -0.429794  ,\n",
      "        0.5373478 ,  0.16965358,  0.0487752 , -0.3915212 ,  0.50425977,\n",
      "        0.6546454 ,  0.5122194 ,  0.1007846 ,  1.2918243 ,  0.03765379,\n",
      "       -0.3297109 , -0.13791364,  0.26570287, -0.10381316,  0.4456462 ,\n",
      "       -0.1843891 ,  0.7306769 , -0.05037342, -0.02183827, -0.5632578 ,\n",
      "       -0.47739217,  0.8803117 ,  0.2917859 ,  0.00569298,  0.6456681 ,\n",
      "        0.2486923 ,  0.33501154, -0.5080212 , -0.36552426,  0.73069656,\n",
      "       -0.5486954 , -0.5631049 ,  0.17190361,  0.19989052, -0.40524167],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Remove empty embeddings (if any) from training data\n",
    "num_removed_train = 0\n",
    "X_train_word_embeddings_filtered = []\n",
    "for embedding in X_train_word_embeddings:\n",
    "    if len(embedding) > 0:\n",
    "        X_train_word_embeddings_filtered.append(embedding)\n",
    "    else:\n",
    "        num_removed_train += 1\n",
    "\n",
    "X_train_word_embeddings = X_train_word_embeddings_filtered\n",
    "\n",
    "# Remove empty embeddings (if any) from testing data\n",
    "num_removed_test = 0\n",
    "X_test_word_embeddings_filtered = []\n",
    "for embedding in X_test_word_embeddings:\n",
    "    if len(embedding) > 0:\n",
    "        X_test_word_embeddings_filtered.append(embedding)\n",
    "    else:\n",
    "        num_removed_test += 1\n",
    "\n",
    "X_test_word_embeddings = X_test_word_embeddings_filtered\n",
    "\n",
    "# Print the number of removed embeddings\n",
    "print(\"Number of removed embeddings (training data):\", num_removed_train)\n",
    "print(\"Number of removed embeddings (testing data):\", num_removed_test)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('--------------X_train_word_embeddings: \\n')\n",
    "print(X_train_word_embeddings[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3fcae6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------X_test_word_embeddings: \n",
      "\n",
      "[array([ 0.26774868,  0.45513487, -0.1283196 , -0.6166905 , -0.4742356 ,\n",
      "       -0.45482776,  0.3290812 ,  0.32271448, -0.3311331 , -0.43133664,\n",
      "        0.6026132 ,  0.36143342,  0.5340214 , -0.08719439, -0.35256717,\n",
      "       -0.0625425 ,  0.287423  , -0.36039275, -0.36010265, -0.43359667,\n",
      "        0.18733892, -0.13334942,  0.03829688, -0.32266244, -0.910652  ,\n",
      "        0.2164441 , -0.10206286,  0.03575484, -0.02808089,  0.10100013,\n",
      "        0.21810633, -0.36831638,  0.35649237, -0.78378   ,  0.19911186,\n",
      "        0.6383603 ,  0.37079847, -0.10995406, -0.32926652, -0.30819955,\n",
      "        0.19594997,  0.15089722, -0.08151383, -0.03778863,  0.885269  ,\n",
      "        0.09953274,  0.26436773,  0.05248537, -0.04592742, -0.14945155,\n",
      "        0.24132542,  0.21955483, -0.12329152,  0.16020009, -0.60332793,\n",
      "       -0.42134184,  0.35439676,  0.78711534, -0.44186175, -0.6303511 ,\n",
      "       -0.23340324, -0.5009944 ,  0.48402983,  0.15878691, -0.25607815,\n",
      "        0.71669984,  0.20963676, -0.05157369, -0.7677294 ,  0.35213727,\n",
      "        0.04998944,  0.6563184 ,  0.21822163,  0.57913756,  0.33971745,\n",
      "       -0.05624944,  0.02300992,  0.47947142, -0.1299305 ,  0.10109831,\n",
      "       -0.2910928 ,  0.24715911, -0.28334042,  0.08362991, -0.729037  ,\n",
      "       -0.0846964 ,  0.38541   , -0.14742701, -0.00126388,  0.47752452,\n",
      "        0.18715651,  0.54333484, -0.36795554, -0.17899957,  0.37785053,\n",
      "       -0.5275348 , -0.42804337,  0.20941748,  0.2726285 , -0.3285808 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print('--------------X_test_word_embeddings: \\n')\n",
    "print(X_test_word_embeddings[:1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "214980bd",
   "metadata": {},
   "source": [
    "## Pad sequences to ensure equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb7c3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word embeddings to numpy arrays\n",
    "X_train_word_embeddings = np.array(X_train_word_embeddings)\n",
    "X_test_word_embeddings = np.array(X_test_word_embeddings)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_sequence_length = max_review_length  # Use the maximum length of a review as the sequence length\n",
    "\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_word_embeddings,\n",
    "    maxlen=max_sequence_length,\n",
    "    dtype='float32',\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_word_embeddings,\n",
    "    maxlen=max_sequence_length,\n",
    "    dtype='float32',\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "# Reshape word embeddings\n",
    "embedding_size = 100  # Set the desired embedding size\n",
    "max_sequence_length = 100  # Set the desired sequence length\n",
    "\n",
    "def reshape_embeddings(embeddings):\n",
    "    reshaped_embeddings = np.zeros((len(embeddings), max_sequence_length, embedding_size))\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # Determine the length of the embedding and truncate if necessary\n",
    "        length = min(len(embedding), max_sequence_length)\n",
    "        reshaped_embeddings[i, :length] = embedding[:length]\n",
    "    return reshaped_embeddings\n",
    "\n",
    "X_train_reshaped = reshape_embeddings(X_train_word_embeddings)\n",
    "X_test_reshaped = reshape_embeddings(X_test_word_embeddings)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e5a24c",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2fa0278",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks is a type of recurrent neural network (RNN) architecture commonly used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "34332c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Model  Test_Loss  Test_Accuracy\n",
      "0                         Simple Model        0.0            0.0\n",
      "1  Increased the number of LSTM layers        0.0            0.0\n",
      "2          With dropout regularization        0.0            0.0\n",
      "3                  Bidirectional LSTMs        0.0            0.0\n",
      "4                     Ensemble methods        0.0            0.0\n"
     ]
    }
   ],
   "source": [
    "# Define a list of model names and their corresponding test loss and test accuracy\n",
    "model_names = ['Simple Model', 'Increased the number of LSTM layers', \n",
    "               'With dropout regularization', 'Bidirectional LSTMs', \n",
    "               'Ensemble methods']\n",
    "test_loss = np.zeros(5)\n",
    "test_accuracy = np.zeros(5)\n",
    "\n",
    "# Create a DataFrame with the model names, test loss, and test accuracy\n",
    "results_df = pd.DataFrame({'Model': model_names, 'Test_Loss': test_loss, 'Test_Accuracy': test_accuracy})\n",
    "\n",
    "# Display the results table\n",
    "print(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "514e7a33",
   "metadata": {},
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e81d6053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 71s 108ms/step - loss: 0.3539 - accuracy: 0.8457 - val_loss: 0.3422 - val_accuracy: 0.8523 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 65s 105ms/step - loss: 0.3269 - accuracy: 0.8588 - val_loss: 0.3421 - val_accuracy: 0.8531 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 67s 108ms/step - loss: 0.3228 - accuracy: 0.8607 - val_loss: 0.3322 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 67s 107ms/step - loss: 0.3157 - accuracy: 0.8652 - val_loss: 0.3324 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 67s 108ms/step - loss: 0.3122 - accuracy: 0.8668 - val_loss: 0.3421 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 66s 107ms/step - loss: 0.3087 - accuracy: 0.8683 - val_loss: 0.3304 - val_accuracy: 0.8564 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 68s 110ms/step - loss: 0.3043 - accuracy: 0.8689 - val_loss: 0.3477 - val_accuracy: 0.8522 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 67s 107ms/step - loss: 0.3008 - accuracy: 0.8715 - val_loss: 0.3297 - val_accuracy: 0.8565 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 66s 106ms/step - loss: 0.2964 - accuracy: 0.8730 - val_loss: 0.3398 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3398 - accuracy: 0.8492\n",
      "Test loss: 0.33980506658554077\n",
      "Test accuracy: 0.8492487668991089\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(max_sequence_length, embedding_size)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss stops improving\n",
    "# the EarlyStopping callback is used to stop the training process \n",
    "# when the validation accuracy does not improve within a certain number of epochs. \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0) \n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5) \n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n",
    "          epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fc822b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test_Loss</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Model</td>\n",
       "      <td>0.339805</td>\n",
       "      <td>0.849249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increased the number of LSTM layers</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With dropout regularization</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bidirectional LSTMs</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble methods</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Test_Loss  Test_Accuracy\n",
       "0                         Simple Model   0.339805       0.849249\n",
       "1  Increased the number of LSTM layers   0.000000       0.000000\n",
       "2          With dropout regularization   0.000000       0.000000\n",
       "3                  Bidirectional LSTMs   0.000000       0.000000\n",
       "4                     Ensemble methods   0.000000       0.000000"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Test_Loss'][results_df['Model']=='Simple Model'] =  loss\n",
    "results_df['Test_Accuracy'][results_df['Model']=='Simple Model'] = accuracy\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "188519dc",
   "metadata": {},
   "source": [
    "#### Increased the number of LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b938506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 156s 244ms/step - loss: 0.3533 - accuracy: 0.8462 - val_loss: 0.3426 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 146s 235ms/step - loss: 0.3290 - accuracy: 0.8576 - val_loss: 0.3353 - val_accuracy: 0.8557 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 147s 237ms/step - loss: 0.3222 - accuracy: 0.8620 - val_loss: 0.3316 - val_accuracy: 0.8610 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 147s 236ms/step - loss: 0.3177 - accuracy: 0.8641 - val_loss: 0.3396 - val_accuracy: 0.8564 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 148s 239ms/step - loss: 0.3122 - accuracy: 0.8641 - val_loss: 0.3337 - val_accuracy: 0.8551 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 147s 237ms/step - loss: 0.3068 - accuracy: 0.8700 - val_loss: 0.3326 - val_accuracy: 0.8553 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 145s 234ms/step - loss: 0.3027 - accuracy: 0.8710 - val_loss: 0.3330 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 145s 234ms/step - loss: 0.2983 - accuracy: 0.8725 - val_loss: 0.3336 - val_accuracy: 0.8576 - lr: 0.0010\n",
      "310/310 [==============================] - 13s 42ms/step - loss: 0.3336 - accuracy: 0.8576\n",
      "Test loss: 0.3335508108139038\n",
      "Test accuracy: 0.8576182126998901\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(max_sequence_length, embedding_size), return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss stops improving\n",
    "# the EarlyStopping callback is used to stop the training process \n",
    "# when the validation accuracy does not improve within a certain number of epochs. \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0) \n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5) \n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n",
    "          epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6b0b3951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test_Loss</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Model</td>\n",
       "      <td>0.339805</td>\n",
       "      <td>0.849249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increased the number of LSTM layers</td>\n",
       "      <td>0.333551</td>\n",
       "      <td>0.857618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With dropout regularization</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bidirectional LSTMs</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble methods</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Test_Loss  Test_Accuracy\n",
       "0                         Simple Model   0.339805       0.849249\n",
       "1  Increased the number of LSTM layers   0.333551       0.857618\n",
       "2          With dropout regularization   0.000000       0.000000\n",
       "3                  Bidirectional LSTMs   0.000000       0.000000\n",
       "4                     Ensemble methods   0.000000       0.000000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Test_Loss'][results_df['Model']=='Increased the number of LSTM layers'] = loss\n",
    "results_df['Test_Accuracy'][results_df['Model']=='Increased the number of LSTM layers'] = accuracy\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bdc40f8",
   "metadata": {},
   "source": [
    "#### With dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8695d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 79s 122ms/step - loss: 0.3594 - accuracy: 0.8422 - val_loss: 0.3434 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 70s 113ms/step - loss: 0.3308 - accuracy: 0.8568 - val_loss: 0.3372 - val_accuracy: 0.8553 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 68s 109ms/step - loss: 0.3228 - accuracy: 0.8614 - val_loss: 0.3357 - val_accuracy: 0.8546 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 67s 109ms/step - loss: 0.3173 - accuracy: 0.8648 - val_loss: 0.3330 - val_accuracy: 0.8565 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 67s 107ms/step - loss: 0.3144 - accuracy: 0.8655 - val_loss: 0.3330 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 67s 107ms/step - loss: 0.3125 - accuracy: 0.8670 - val_loss: 0.3292 - val_accuracy: 0.8599 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 66s 107ms/step - loss: 0.3082 - accuracy: 0.8691 - val_loss: 0.3375 - val_accuracy: 0.8575 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 66s 106ms/step - loss: 0.3054 - accuracy: 0.8693 - val_loss: 0.3314 - val_accuracy: 0.8581 - lr: 0.0010\n",
      "310/310 [==============================] - 7s 22ms/step - loss: 0.3314 - accuracy: 0.8581\n",
      "Test loss: 0.3314211964607239\n",
      "Test accuracy: 0.8581224083900452\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(max_sequence_length, embedding_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss stops improving\n",
    "# the EarlyStopping callback is used to stop the training process \n",
    "# when the validation accuracy does not improve within a certain number of epochs. \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, cooldown=0) \n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=3) \n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n",
    "          epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6fb0fdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test_Loss</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Model</td>\n",
       "      <td>0.339805</td>\n",
       "      <td>0.849249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increased the number of LSTM layers</td>\n",
       "      <td>0.333551</td>\n",
       "      <td>0.857618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With dropout regularization</td>\n",
       "      <td>0.331421</td>\n",
       "      <td>0.858122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bidirectional LSTMs</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble methods</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Test_Loss  Test_Accuracy\n",
       "0                         Simple Model   0.339805       0.849249\n",
       "1  Increased the number of LSTM layers   0.333551       0.857618\n",
       "2          With dropout regularization   0.331421       0.858122\n",
       "3                  Bidirectional LSTMs   0.000000       0.000000\n",
       "4                     Ensemble methods   0.000000       0.000000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Test_Loss'][results_df['Model']=='With dropout regularization'] = loss\n",
    "results_df['Test_Accuracy'][results_df['Model']=='With dropout regularization'] = accuracy\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "594e484f",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9406b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 [==============================] - 298s 458ms/step - loss: 0.3532 - accuracy: 0.8453 - val_loss: 0.3431 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 286s 461ms/step - loss: 0.3297 - accuracy: 0.8581 - val_loss: 0.3516 - val_accuracy: 0.8498 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 273s 440ms/step - loss: 0.3224 - accuracy: 0.8623 - val_loss: 0.3328 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 272s 438ms/step - loss: 0.3153 - accuracy: 0.8643 - val_loss: 0.3299 - val_accuracy: 0.8581 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 271s 437ms/step - loss: 0.3108 - accuracy: 0.8665 - val_loss: 0.3408 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 279s 451ms/step - loss: 0.3063 - accuracy: 0.8683 - val_loss: 0.3341 - val_accuracy: 0.8562 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 274s 441ms/step - loss: 0.3032 - accuracy: 0.8701 - val_loss: 0.3325 - val_accuracy: 0.8579 - lr: 0.0010\n",
      "310/310 [==============================] - 18s 57ms/step - loss: 0.3325 - accuracy: 0.8579\n",
      "Test loss: 0.33250999450683594\n",
      "Test accuracy: 0.85792076587677\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(100, input_shape=(max_sequence_length, embedding_size), return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss stops improving\n",
    "# the EarlyStopping callback is used to stop the training process \n",
    "# when the validation accuracy does not improve within a certain number of epochs. \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, cooldown=0) \n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=3) \n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n",
    "          epochs=10, batch_size=64, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "823a806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test_Loss</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Model</td>\n",
       "      <td>0.339805</td>\n",
       "      <td>0.849249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Increased the number of LSTM layers</td>\n",
       "      <td>0.333551</td>\n",
       "      <td>0.857618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With dropout regularization</td>\n",
       "      <td>0.331421</td>\n",
       "      <td>0.858122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bidirectional LSTMs</td>\n",
       "      <td>0.332510</td>\n",
       "      <td>0.857921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble methods</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Test_Loss  Test_Accuracy\n",
       "0                         Simple Model   0.339805       0.849249\n",
       "1  Increased the number of LSTM layers   0.333551       0.857618\n",
       "2          With dropout regularization   0.331421       0.858122\n",
       "3                  Bidirectional LSTMs   0.332510       0.857921\n",
       "4                     Ensemble methods   0.000000       0.000000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Test_Loss'][results_df['Model']=='Bidirectional LSTMs'] = loss\n",
    "results_df['Test_Accuracy'][results_df['Model']=='Bidirectional LSTMs'] = accuracy\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab61f8ed",
   "metadata": {},
   "source": [
    "#### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1a59a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df['Test_Loss'][results_df['Model']=='Ensemble methods'] = loss\n",
    "# results_df['Test_Accuracy'][results_df['Model']=='Ensemble methods'] = accuracy\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "062e5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results_df into new csv file\n",
    "results_df[['Model', 'Test_Loss', 'Test_Accuracy']].to_csv('LSTM_model_results.csv', index=False, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65b94e1a",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7a4ba5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a neural network model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add a fully connected layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(avg_vector_size,)))\n",
    "\n",
    "# # Add another fully connected layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add a final output layer with 1 unit and sigmoid activation for binary classification\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test), epochs=10, batch_size=64)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test loss: {loss:.2f}')\n",
    "# print(f'Test accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
